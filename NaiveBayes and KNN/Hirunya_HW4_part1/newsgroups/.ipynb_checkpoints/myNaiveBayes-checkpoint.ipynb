{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataframe\n",
    "myTrain = pd.read_csv('trainClasses.txt',sep = \"\\t\",header = None)\n",
    "\n",
    "myTest  = pd.read_csv('testClasses.txt',sep = \"\\t\",header = None)\n",
    "\n",
    "myTrainMatrix = pd.read_csv('trainMatrix.txt',sep = \"\\t\",header = None)\n",
    "\n",
    "myTestMatrix  = pd.read_csv('testMatrix.txt',sep = \"\\t\",header = None)\n",
    "myTerms     =  pd.read_csv('terms.txt',sep = \"\\t\",header = None, keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(myTerms.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train matrix\n",
    "myTrain_df = myTrainMatrix.T #transpose array\n",
    "myTrain_df.columns = vocabulary\n",
    "myTrain_df['CLASS'] = myTrain.loc[:,1].values\n",
    "myTrain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test matrix\n",
    "myTest_df = myTestMatrix.T #transpose array\n",
    "myTest_df.columns = vocabulary\n",
    "myTest_df['CLASS'] = myTest.loc[:,1].values\n",
    "myTest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Probabilities and counts for words / classes\n",
    " #train component according to slide 78\n",
    "V = len(vocabulary)\n",
    "prob_windows = myTrain_df['CLASS'].value_counts(normalize=True)[0]\n",
    "prob_hockey = myTrain_df['CLASS'].value_counts(normalize=True)[1]\n",
    "n_windows = myTrain_df[myTrain_df['CLASS'] == 0].sum().sum()\n",
    "n_hockey = myTrain_df[myTrain_df['CLASS'] == 1].sum().sum()\n",
    "print(V, prob_windows, prob_hockey, n_windows, n_hockey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTrain_df['CLASS'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = myTrain_df['CLASS'] == 0\n",
    "myTrain_df[mask].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_prob_given_windows = {word: 0 for word in vocabulary}\n",
    "word_prob_given_hockey = {word: 0 for word in vocabulary}\n",
    "\n",
    "windows_df = myTrain_df[myTrain_df['CLASS'] == 0].copy()\n",
    "hockey_df = myTrain_df[myTrain_df['CLASS'] == 1].copy()\n",
    "\n",
    "\n",
    "for word in vocabulary:\n",
    "    word_prob_given_windows[word] = (windows_df[word].sum() +1) / (n_windows + V)\n",
    "    word_prob_given_hockey[word] = (hockey_df[word].sum() +1) / (n_hockey + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test component according to the slide 79\n",
    "row = myTest_df.loc[1]\n",
    "windows_prob_given_message = np.log(prob_windows)\n",
    "hockey_prob_given_message = np.log(prob_hockey)\n",
    "\n",
    "for word in vocabulary:\n",
    "    if row[word] >0: #whenever words show zero time - filter only words are show more than one\n",
    "        windows_prob_given_message = windows_prob_given_message + row[word] + np.log(word_prob_given_windows[word])\n",
    "        hockey_prob_given_message = hockey_prob_given_message + row[word] + np.log(word_prob_given_hockey[word])\n",
    "\n",
    "print(windows_prob_given_message, hockey_prob_given_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifire working for classification the class probability.\n",
    "def classifier(row):\n",
    "    windows_prob_given_message = np.log(prob_windows)\n",
    "    hockey_prob_given_message = np.log(prob_hockey)\n",
    "\n",
    "    for word in vocabulary:\n",
    "        if row[word] >0:\n",
    "            windows_prob_given_message = windows_prob_given_message + row[word]+ np.log(word_prob_given_windows[word])\n",
    "            hockey_prob_given_message = hockey_prob_given_message + row[word]+ np.log(word_prob_given_hockey[word])\n",
    "\n",
    "    if windows_prob_given_message > hockey_prob_given_message:\n",
    "        return 0\n",
    "    else: \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTest_df['PREDICTED'] = myTest_df.apply(classifier, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for writeup Q1.\n",
    "overall_classification_acc = (myTest_df['CLASS'] == myTest_df['PREDICTED']).sum() / len(myTest_df)*100\n",
    "print('overall classification accuracy = ',  (overall_classification_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Q2.\n",
    "print(myTest_df[['CLASS','PREDICTED']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for writeup Q3.\n",
    "terms = [\"program\", \"includ\", \"match\", \"game\", \"plai\", \"window\", \"file\", \"subject\", \"write\"]\n",
    "\n",
    "# * prob(windows | term)  = prob(term | windows) * p(windows) / p(term)\n",
    "\n",
    "for term in terms:\n",
    "    p_term = myTrain_df[term].sum() / len(myTrain_df)\n",
    "    p_win_giv_term = word_prob_given_windows[term] * prob_windows / p_term\n",
    "    p_hock_giv_term = word_prob_given_hockey[term] * prob_hockey / p_term\n",
    "    print('Word %s,    P(Windows): %.6f ,    P(Hockey): %.6f' %(term, p_win_giv_term, p_hock_giv_term))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(myTest_df['CLASS'], myTest_df['PREDICTED']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
